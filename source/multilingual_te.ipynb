{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "cache_dir = \"/shared/.cache/transformers\"\n",
    "gpu_device = 2\n",
    "te_model = AutoModelForSequenceClassification.from_pretrained('joeddav/xlm-roberta-large-xnli', cache_dir=cache_dir).to('cuda:'+str(gpu_device))\n",
    "tokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli', cache_dir=cache_dir)\n",
    "os.chdir('/shared/lyuqing/probing_for_event/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def entailment(premise, hypothesis):\n",
    "\n",
    "    x = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation='only_first').to('cuda:'+str(gpu_device))\n",
    "    logits = te_model(x)[0]\n",
    "    entail_contradiction_logits = logits\n",
    "    probs = entail_contradiction_logits.softmax(1)\n",
    "    prob_label_is_true = float(probs[:, 2])\n",
    "    return prob_label_is_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_trg_probe_lexicon(fr, level):\n",
    "    lexicon = {}\n",
    "    if level == 'fine':\n",
    "        for line in fr:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if line.isupper():\n",
    "                    event_type = line\n",
    "                else:\n",
    "                    lexicon[event_type] = line\n",
    "                    \n",
    "    return lexicon\n",
    "trg_probes_frn = 'source/lexicon/nli_topics.txt'\n",
    "with open(trg_probes_frn, 'r') as fr:\n",
    "    trg_probe_lexicon = load_trg_probe_lexicon(fr, 'fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict_event(sentence, trg_probe_lexicon):\n",
    "    result_dict = {}\n",
    "    for event_type in trg_probe_lexicon.keys():\n",
    "        label = trg_probe_lexicon[event_type]\n",
    "        hypothesis = f'This text is about {label}.'\n",
    "        premise = sentence\n",
    "        orig_entail_prob = entailment(premise, hypothesis)\n",
    "#         if pair_premise_strategy:\n",
    "#             sub_pattern = '\\s?' + trigger_text + '\\s?'\n",
    "#             truncated_premise = re.sub(pattern=sub_pattern, string=premise, repl=' ').strip()\n",
    "#             truncated_entail_prob = self.entailment(truncated_premise, hypothesis)\n",
    "#             delta = orig_entail_prob - truncated_entail_prob\n",
    "\n",
    "#         if self.pair_premise_strategy == 'max_delta':\n",
    "#             result_dict[event_type] = delta\n",
    "#         elif self.pair_premise_strategy == 'max_conf+delta':\n",
    "#             result_dict[event_type] = orig_entail_prob + delta\n",
    "#         elif self.pair_premise_strategy == None:\n",
    "        result_dict[event_type] = orig_entail_prob\n",
    "\n",
    "    sorted_res = sorted(result_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_type, confidence = sorted_res[0][0], sorted_res[0][1]\n",
    "    \n",
    "    return top_type, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TRANSPORT', 0.9896987080574036)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence = \"击毙反政府武装分子\" # \"Killed the anti-government rebels\"\n",
    "\n",
    "# sentence = \"平民暂时离开家园。\" # \"Civilians temporarily departed from home\"\n",
    "\n",
    "# sentence = \"政府军缴获了一批武器。\" # \"Government forces seized a batch of weapons.\"\n",
    "\n",
    "# sentence = \"政府军方面有１２人受伤\" # \"12 people were injured in the government forces\"\n",
    "\n",
    "# sentence = \"10多年来邻国内战\" # The neighboring country has been in civil war for more than ten years\n",
    "\n",
    "predict_event(sentence, trg_probe_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8199309706687927"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entailment testing\n",
    "sentence = \"10多年来邻国内战\" # The neighboring country has been in civil war for more than ten years\n",
    "hypothesis = \"This text is about an attack or a war.\"\n",
    "entailment(sentence, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "events",
   "language": "python",
   "name": "events"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
